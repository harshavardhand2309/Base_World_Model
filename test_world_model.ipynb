{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfd8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9722da7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VLA_WorldModel:\n\tMissing key(s) in state_dict: \"network.6.weight\", \"network.6.bias\". \n\tUnexpected key(s) in state_dict: \"latent_norm.weight\", \"latent_norm.bias\", \"action_norm.weight\", \"action_norm.bias\", \"text_norm.weight\", \"text_norm.bias\". \n\tsize mismatch for network.4.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for network.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    175\u001b[39m     DEVICE = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     tester = \u001b[43mVLATester\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvae_vla_base.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mworld_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# Test Case 1: Load a specific episode from your data\u001b[39;00m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Change 'ep_0' to an episode that exists in your 'drawing_data' folder\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mVLATester.__init__\u001b[39m\u001b[34m(self, vae_path, wm_path, device)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.vae.eval()\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.wm = VLA_WorldModel().to(device)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwm_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m.wm.eval()\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.clip = CLIPHandler(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Base-World-Model\\venv_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for VLA_WorldModel:\n\tMissing key(s) in state_dict: \"network.6.weight\", \"network.6.bias\". \n\tUnexpected key(s) in state_dict: \"latent_norm.weight\", \"latent_norm.bias\", \"action_norm.weight\", \"action_norm.bias\", \"text_norm.weight\", \"text_norm.bias\". \n\tsize mismatch for network.4.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for network.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256])."
     ]
    }
   ],
   "source": [
    "\n",
    "class VAE64(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAE64, self).__init__()\n",
    "        \n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(512, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)\n",
    "        \n",
    "        # Decoder: 1x1 -> 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.decoder_input = nn.Linear(latent_dim, 512)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (512, 1, 1)),\n",
    "            nn.ConvTranspose2d(512, 128, 8, stride=1, padding=0), # 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # 32x32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),    # 64x64\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_input(z)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "class VLA_WorldModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128, action_dim=3, text_embed_dim=512):\n",
    "        super(VLA_WorldModel, self).__init__()\n",
    "        \n",
    "        # Combined Input: Latent (128) + Action (3) + Text (512) = 643\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim + text_embed_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, latent_dim) # Predicts next Z\n",
    "        )\n",
    "\n",
    "    def forward(self, z, action, text_embedding):\n",
    "        # Concatenate everything into one long vector\n",
    "        # z: (batch, 128), action: (batch, 3), text_embedding: (batch, 512)\n",
    "        x = torch.cat([z, action, text_embedding], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class CLIPHandler:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # Freeze CLIP weights (we only want to use it, not train it)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def embed_text(self, text_list):\n",
    "        inputs = self.tokenizer(text_list, padding=True, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        # We use the pooled output (final representation of the sentence)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# --- 2. The Tester Engine ---\n",
    "\n",
    "class VLATester:\n",
    "    def __init__(self, vae_path, wm_path, device):\n",
    "        self.device = device\n",
    "        self.vae = VAE64().to(device)\n",
    "        self.vae.load_state_dict(torch.load(vae_path, map_location=device))\n",
    "        self.vae.eval()\n",
    "\n",
    "        self.wm = VLA_WorldModel().to(device)\n",
    "        self.wm.load_state_dict(torch.load(wm_path, map_location=device))\n",
    "        self.wm.eval()\n",
    "\n",
    "        self.clip = CLIPHandler(device)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(), transforms.Resize((64, 64)), transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def test_sequence(self, episode_path, num_steps=5):\n",
    "        \"\"\" Tests if the model can predict a sequence of actions accurately. \"\"\"\n",
    "        \n",
    "        # Load the meta data (Instruction and Actions)\n",
    "        with open(os.path.join(episode_path, \"meta.json\"), \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        \n",
    "        instruction = meta[\"instruction\"]\n",
    "        actions = meta[\"actions\"][:num_steps]\n",
    "        \n",
    "        # Load the initial frame (T=0)\n",
    "        frames = sorted(glob.glob(os.path.join(episode_path, \"*.png\")))\n",
    "        img_start = cv2.imread(frames[0], cv2.IMREAD_GRAYSCALE)\n",
    "        img_tensor = self.transform(img_start).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Get Text Embedding\n",
    "        text_embed = self.clip.embed_text(instruction)\n",
    "\n",
    "        # Storage for results\n",
    "        predicted_images = []\n",
    "        actual_images = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Initial Latent\n",
    "            z, _ = self.vae.encode(img_tensor)\n",
    "\n",
    "            for i in range(num_steps):\n",
    "                # 1. Prediction step\n",
    "                action_tensor = torch.tensor([actions[i]], dtype=torch.float32).to(self.device)\n",
    "                z = self.wm(z, action_tensor, text_embed)\n",
    "                \n",
    "                # 2. Decode Predicted Latent\n",
    "                pred_img = self.vae.decode(z).squeeze().cpu().numpy()\n",
    "                predicted_images.append(pred_img)\n",
    "\n",
    "                # 3. Load Actual Frame for Comparison\n",
    "                actual_img = cv2.imread(frames[i+1], cv2.IMREAD_GRAYSCALE)\n",
    "                actual_img = cv2.resize(actual_img, (64, 64)) / 255.0\n",
    "                actual_images.append(actual_img)\n",
    "\n",
    "        self.visualize(instruction, actual_images, predicted_images)\n",
    "\n",
    "    def visualize(self, instruction, actuals, preds):\n",
    "        n = len(actuals)\n",
    "        fig, axes = plt.subplots(2, n, figsize=(15, 6))\n",
    "        for i in range(n):\n",
    "            axes[0, i].imshow(actuals[i], cmap='gray')\n",
    "            axes[0, i].set_title(f\"Target T+{i+1}\")\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "            axes[1, i].imshow(preds[i], cmap='gray')\n",
    "            axes[1, i].set_title(f\"Dream T+{i+1}\")\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f\"Instruction: {instruction}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- 3. Run Test Cases ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tester = VLATester(\"vae_vla_base.pth\", \"world_model.pth\", DEVICE)\n",
    "\n",
    "    # Test Case 1: Load a specific episode from your data\n",
    "    # Change 'ep_0' to an episode that exists in your 'drawing_data' folder\n",
    "    try:\n",
    "        tester.test_sequence(\"drawing_data/ep_0\", num_steps=5)\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {e}. Check if 'drawing_data/ep_0' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efa76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
